!pip install emoji
import re
import pandas as pd
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
import emoji
import spacy
from sklearn.model_selection import train_test_split
import nltk
nltk.download('vader_lexicon')
nltk.download('words')
nltk.download('stopwords')
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np

from google.colab import drive
drive.mount('/content/drive')
par1 = pd.read_csv('/content/drive/MyDrive/BNB_data1.csv')
par3 = pd.read_csv('/content/drive/MyDrive/usdt_data3.csv')
par4 = pd.read_csv('/content/drive/MyDrive/BTC.csv')
par5 = pd.read_csv('/content/drive/MyDrive/BNB_data2.csv')
df = pd.concat([par1, par3, par4, par5])

nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

for i in range (len(df)):
  df.iloc[i,2] = str(df.iloc[i,2])
df

# filter out languages other than English
for i in range (len(df)):
  df.iloc[i,2] = ' '.join(re.sub('[^A-Za-z0-9]',' ', df.iloc[i,2]).split())
  
# convert emojis to words
for i in range (len(df)):
  df.iloc[i,2] = emoji.demojize(df.iloc[i,2])
  df.iloc[i,2] = df.iloc[i,2].replace(":"," ")
  df.iloc[i,2] = ' '.join(df.iloc[i,2].split())
  
# remove urls
for i in range (len(df)):
  df.iloc[i,2] = ' '.join(re.sub("(\w+:\/\/\S+)", " ", df.iloc[i,2]).split())

# change to lowercase letters
df['new_Text'] = df['Text'].apply(lambda x: " ".join(x.lower() for x in x.split()))
df['new_Text'].head()

# remove special chracters
df['new_Text'] = df['new_Text'].str.replace('[^\w\s]','')
df['new_Text'].head()

stop = stopwords.words('english')
df['new_Text'] = df['new_Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
df.head(20)

def space(comment):
    doc = nlp(comment)
    return " ".join([token.lemma_ for token in doc])
df['new_Text']= df['new_Text'].apply(space)
df.head()

def get_sentence_mean_vector(morphs):
    vector = []
    for i in morphs:
        try:
            vector.append(models.wv[i])
        except KeyError as e:
            pass
    try:
        return np.mean(vector, axis=0)
    except IndexError as e:
        pass

# install word2vec 
!pip install gensim
from gensim.models import Word2Vec
models = Word2Vec(df['new_Text'], size=8, window=3, negative=3, min_count=3, workers=3, sg=1)

df['wv'] = df['new_Text'].map(get_sentence_mean_vector)

df, df_test = train_test_split(df, test_size=0.2, random_state=7)

from sklearn.cluster import KMeans
import time

word_vectors = df.wv.to_list() 
num_clusters = 3
word_vectors[:10]
word_vectors = list(word_vectors)
kmeans_clustering = KMeans(n_clusters = num_clusters)
idx = kmeans_clustering.fit_predict(word_vectors)
df = df.drop(df.index[174002])
df['category'] = idx
df['category']
df[['new_Text','wv']]

from sklearn.manifold import TSNE

X = df['wv'].to_list()

y = df['category'].to_list()

import os.path
import pickle

tsne_filepath = 'tsne3000.pkl'

# File Cache
if not os.path.exists(tsne_filepath):
    tsne = TSNE(random_state=41)
    tsne_points = tsne.fit_transform(X)
    with open(tsne_filepath, 'wb+') as f:
        pickle.dump(tsne_points, f)
else: # Cache Hits!
    with open(tsne_filepath, 'rb') as f:
        tsne_points = pickle.load(f)

tsne_df = pd.DataFrame(tsne_points, index=range(len(X)), columns=['x_coord', 'y_coord'])
tsne_df['tweets'] = df['new_Text'].to_list()
tsne_df['cluster_no'] = y

groups = tsne_df.groupby('cluster_no')

fig, ax = plt.subplots()

for name, group in groups:

    ax.plot(group.x_coord, 
            group.y_coord, 
            marker='o', 
            linestyle='',
            label=name)

ax.legend(fontsize=12, loc='upper right') # legend position
plt.title('Scatter Plot of wordvector by matplotlib', fontsize=20)
plt.xlabel('X_coord', fontsize=14)
plt.ylabel('Y_coord', fontsize=14)
plt.show()

cl0 = tsne_df.loc[tsne_df['cluster_no']==0]
cl1 = tsne_df.loc[tsne_df['cluster_no']==1]
cl2 = tsne_df.loc[tsne_df['cluster_no']==2]

para =min(len(cl2),len(cl1),len(cl0))

part1 = cl1.sample(n=para)
part2 = cl2.sample(n=para)
part0 = cl0.sample(n=para)

tsne_df = pd.concat([part1, part2, part0])

from sklearn import svm
X = tsne_df[['x_coord','y_coord']]
y = tsne_df['cluster_no']

model = svm.SVC(gamma = 0.5 , C = 0.5)
model.fit(X,y)

from sklearn.manifold import TSNE

word_vectors2 = df_test.wv.to_list()
df_test['wv'] = word_vectors2 

X2 = df_test['wv'].to_list()

import os.path
import pickle

tsne_filepath = 'tsne3001.pkl'

# File Cache
if not os.path.exists(tsne_filepath):
    tsne = TSNE(random_state=41)
    tsne_points2 = tsne.fit_transform(X2)
    with open(tsne_filepath, 'wb+') as f:
        pickle.dump(tsne_points2, f)
else: # Cache Hits!
    with open(tsne_filepath, 'rb') as f:
        tsne_points2 = pickle.load(f)

tsne_df2 = pd.DataFrame(tsne_points2, index=range(len(X2)), columns=['x_coord', 'y_coord'])

tsne_df2

X_test = tsne_df2[['x_coord','y_coord']]
y_predict = model.predict(X_test)
df_test['cluster_no'] = y_predict

df_test = df_test[['Datetime','cluster_no']]
df_test.describe()

# greed = 1, neutral = 0, fear = -1 
for i in range (len(df_test)):
  if df_test.iloc[i,-1] == 0 :
    df_test.iloc[i,-1] = 1
  elif df_test.iloc[i,-1] == 1:
    df_test.iloc[i,-1] = -1
  else:
    df_test.iloc[i,-1] = 0
    
df_test.describe()
df_test.sort_index()
  
for i in range (len(df_test)):
df_test.iloc[i,0] = df_test.iloc[i,0][0:10]
  
df_final = df_test.groupby('Datetime').sum() 
  
for i in range(len(df_final)):
  if len(df_final.index[i]) != 10:
    df_final.drop(df_final.index[i])  
else:
  if df_final.index[i][:4] != '2021':
    df_final.drop(df_final.index[i])
  
df_final
  
len_date = df_test.groupby('Datetime').size().to_list()
for i in range(len(df_final)):
  df_final.iloc[i,0] = df_final.iloc[i,0] / len_date[i]
df_final

label = pd.read_csv('/content/drive/MyDrive/crypto_fg.csv')
label
type(label.iloc[1,1])

import datetime
format = '%m-%d-%Y'
for i in range(len(label)):
  label.iloc[i,0] = dt_datetime = datetime.datetime.strptime(label.iloc[i,0],format)
  label.iloc[i,0] = str(label.iloc[i,0])
for i in range (len(label)):
  label.iloc[i,0] = label.iloc[i,0][0:10]
label

from google.colab import files
import csv

df_final.to_csv('df_final.csv', index = False, encoding = 'utf-8')
files.download('df_final.csv')

label.to_csv('label.csv', index = False, encoding = 'utf-8')
files.download('label.csv')

df_final = pd.DataFrame(df_final[10:196])

label.sort_index(ascending = True, inplace=True)
label[:5]

label.loc[label['Datetime']=='2021-06-26']
label.loc[label['Datetime']=='2021-12-31']
label = pd.DataFrame(label[2:190])
label.sort_index(ascending= False, inplace = True)
label
label.set_index('Datetime')

for i in range(len(df_final)):
  if df_final.iloc[i,0] > 0.03:
    df_final.iloc[i,0] = 'Fear'
  elif df_final.iloc[i,0] < -0.03:
    df_final.iloc[i,0] = 'Greed'
  else:
    df_final.iloc[i,0] = 'Neutral'
df_final    

score = 0
for i in range(180):
  if df_final.iloc[i,0] == label.iloc[i,1]:
    score += 1
score

for k in range(8):
  scores = 0  
  for i in range(180):
    if df_final.iloc[i,0] == label.iloc[i+k,1]:
      scores += 1
  print(f'after {k}days accurarcy is {scores/181}')
  
for i in range(len(df_final2)):
  if df_final2.iloc[i,0] == 'Fear':
    df_final2.iloc[i,0] = 'Greed'
  elif df_final2.iloc[i,0] == 'Greed':
    df_final2.iloc[i,0] = 'Fear'
    
for k in range(8):
  scores = 0  
  for i in range(180):
    if df_final2.iloc[i,0] == label.iloc[i+k,1]:
      scores += 1
  print(f'after {k}days accurarcy is {scores/181}')
